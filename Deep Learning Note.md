# Deep Learning Note

## Information Theory

### KL Divergence

- ?

### Cross Entropy  

- expected prediction information when model is *q* and ground truth is  *p*
- Sigma(p(x)\* log[q(x)])

### Kraft-McMillan Theorem



### Entropy

- expected information to gain from a event
-  Sigma( p(x)\*log[ 1/p(x) ] ) = - Sigma( p(x)\*log[ p(x) ] )
- â€‹

 



## Reinforcement Learning

