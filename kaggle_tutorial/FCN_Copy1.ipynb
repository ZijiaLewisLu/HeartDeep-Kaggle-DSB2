{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# A Fully Convolutional Network for Left Ventricle Segmentation\n",
    "### by Vu Tran, Data Scientist, Booz Allen Hamilton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dicom, lmdb, cv2, re, sys\n",
    "import os, fnmatch, shutil, subprocess\n",
    "from IPython.utils import io\n",
    "import numpy as np\n",
    "np.random.seed(1234)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') # we ignore a RuntimeWarning produced from dividing by zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully imported packages, hooray!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CAFFE_ROOT = \"/home/zijia/caffe_FCN/\"\n",
    "caffe_path = os.path.join(CAFFE_ROOT, \"python\")\n",
    "if caffe_path not in sys.path:\n",
    "    sys.path.insert(0, caffe_path)\n",
    "\n",
    "import caffe\n",
    "\n",
    "print(\"\\nSuccessfully imported packages, hooray!\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "caffe.set_mode_gpu() # or caffe.set_mode_cpu() for machines without a GPU\n",
    "try:\n",
    "    del solver # it is a good idea to delete the solver object to free up memory before instantiating another one\n",
    "    solver = caffe.SGDSolver('fcn_solver.prototxt')\n",
    "except NameError:\n",
    "    solver = caffe.SGDSolver('fcn_solver.prototxt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# each blob has dimensions batch_size x channel_dim x height x width\n",
    "[(k, v.data.shape) for k, v in solver.net.blobs.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "blb = solver.net.blobs\n",
    "print blb['label']\n",
    "l_blob = blb['label']\n",
    "print l_blob.data.shape\n",
    "print l_blob.channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we inspect the parameter blobs, which store the model weights. Note that all layers with learnable weights have the `weight_filler` configuration; it is necessary to (randomly) initialize weights before the start of training and update them during the training process through gradient descent. Each parameter blob is updated using a diff blob that has the same dimensions. A diff blob stores the gradient of the loss function computed by the network with respect to the corresponding data blob during backward propagation of errors (backprop). In addition, access to diff blobs is useful for at least two purposes: (1) model debugging and diagnosis - a model with zero diffs does not compute gradients and hence does not learn anything, which may indicate a vanishing-gradient problem; (2) visualization of class saliency maps for input images, as suggested in the paper [Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps](http://arxiv.org/pdf/1312.6034.pdf).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print the layers with learnable weights and their dimensions\n",
    "[(k, v[0].data.shape) for k, v in solver.net.params.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# print the biases associated with the weights\n",
    "[(k, v[1].data.shape) for k, v in solver.net.params.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# params and diffs have the same dimensions\n",
    "[(k, v[0].diff.shape) for k, v in solver.net.params.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, before we begin solving our nets it is a good idea to verify that the nets correctly load the data and that gradients are propagating through the filters to update the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': array(0.0008087158203125, dtype=float32),\n",
       " 'loss': array(0.6931471824645996, dtype=float32)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass with randomly initialized weights\n",
    "solver.net.forward()  # train net\n",
    "solver.test_nets[0].forward()  # test net (more than one net is supported)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss']\n"
     ]
    }
   ],
   "source": [
    "p_img = solver.net.\n",
    "print p_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def iou(solver):\n",
    "    solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_train = solver.net.blobs['data'].data[0,0,...]\n",
    "img_test = solver.test_nets[0].blobs['data'].data[0,0,...]\n",
    "\n",
    "form = \"IOU/{0}\"\n",
    "\n",
    "score.seg_tests(solver, form , range(10) , layer='score', gt= \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# visualize the image data and its correpsonding label from the train net\n",
    "img_train = solver.net.blobs['data'].data[0,0,...]\n",
    "plt.imshow(img_train)\n",
    "plt.show()\n",
    "label_train = solver.net.blobs['label'].data[0,0,...]\n",
    "plt.imshow(label_train)\n",
    "plt.show()\n",
    "\n",
    "# visualize the image data and its correpsonding label from the test net\n",
    "img_test = solver.test_nets[0].blobs['data'].data[0,0,...]\n",
    "plt.imshow(img_test)\n",
    "plt.show()\n",
    "label_test = solver.test_nets[0].blobs['label'].data[0,0,...]\n",
    "plt.imshow(label_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# take one step of stochastic gradient descent consisting of both forward pass and backprop\n",
    "solver.step(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# visualize gradients after backprop. If non-zero, then gradients are properly propagating and the nets are learning something\n",
    "# gradients are shown here as 10 x 10 grid of 5 x 5 filters\n",
    "plt.imshow(solver.net.params['conv1'][0].diff[:,0,...].reshape(10,10,5,5).transpose(0,2,1,3).reshape(10*5,10*5), 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like images and labels are correctly loaded in both train and test nets, and gradients are propagating through `conv1` layer, which is the lowest (first) layer of the network. That is certainly good news. Now that we are confident the nets have been properly defined and loaded, let's allow the model to train according to the protocol specified in `fcn_solver.prototxt`. There are two ways to do this:\n",
    "\n",
    "1. Execute the command `solver.solve()` and let the solver take care of the rest. This command logs output to standard output, or in the terminal.\n",
    "2. One can use a Python subprocess module to call the Caffe binary from the shell and redirect the output logs to a file on disk for further analysis. This is the preferred method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "ret = subprocess.call(os.path.join(CAFFE_ROOT, 'build/tools/caffe') + ' ' + 'train -solver=fcn_solver.prototxt -gpu 0 2> fcn_train.log', shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caffe trains the model in about 18 minutes on a MacBook Pro using NVIDIA GeForce GT 750M with 2GB of dedicated GPU memory. The final validation accuracy is 0.996, in terms of pixel-wise binary accuracy, as shown below from my training log:\n",
    "\n",
    "```\n",
    "Iteration 15000, Testing net (#0)\n",
    "Test net output #0: accuracy = 0.996439\n",
    "Test net output #1: loss = 0.0094304 (* 1 = 0.0094304 loss)\n",
    "Optimization Done.\n",
    "```\n",
    "\n",
    "Although we use binary accuracy in this tutorial, be aware that it is a misleading metric to measure performance in this context. In our LV segmentation task, every pixel has an associated class label: 0 for background and 1 for LV. But less than 2 percent of all pixels in the Sunnybrook dataset correspond to the LV class. This is a classic class imbalance problem, where the class distribution is highly skewed and binary accuracy is not a meaningful performance metric. Suppose a model simply predicts all pixels to be background, then its accuracy performance is still greater than 0.98, even though the model is not able to actually detect pixels belonging to the LV object. The reader is encouraged to consider the following alternative performance metrics for LV segmentation: [S&oslash;rensen–Dice index](https://en.wikipedia.org/wiki/S%C3%B8rensen%E2%80%93Dice_coefficient) and [average perpendicular distance](http://smial.sri.utoronto.ca/LV_Challenge/Evaluation.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Apply trained Caffe FCN model to compute EF\n",
    "This step shows how one can apply the Caffe FCN model trained on the Sunnybrook dataset to the DSB dataset for LV segmentation and EF calculation. This process can be considered as a form of *transfer learning*, where the task is to transfer the learned representation from one dataset to another with the goal of detecting similar representation across datasets. In order to proceed with this step, we need two additional files: `fcn_deploy.prototxt` and `fcn_iter_15000.caffemodel`. The file `fcn_deploy.prototxt` is manually derived from the train net file `fcn_train.prototxt` by discarding the `data` layers and converting the `SoftmaxWithLoss` layer to the `Softmax` layer. Once created, the deploy protobuf looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat fcn_deploy.prototxt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we specify the solver protocol to snapshot Caffe binaries to disk at every 2500-step interval. We use the Caffe model at the final snapshot iteration 15000, `fcn_iter_15000.caffemodel`, to perform LV segmentation and compute EF on the DSB dataset. We borrow the following code snippets from [Alex Newton's tutorial](https://www.kaggle.com/c/second-annual-data-science-bowl/details/fourier-based-tutorial) to accomplish this step. First, we load a DSB dataset (e.g., the training set) using the `Dataset` class. The main functionality of the `Dataset` class is to transform the information stored in the DICOM files to a more convenient form in memory. \n",
    "\n",
    "**The fields that we need to calculate EF are:** \n",
    "- `images`, which stores a 4-dimensional NumPy array of intensities (slices x times x height x width); \n",
    "- `area_multiplier`, which represents the ratio between pixels and square micrometers; \n",
    "- `dist`, which is the distance in millimeters between adjacent slices.\n",
    "\n",
    "Note that the DICOM metadata from which `dist` is derived is not always present, so we have to try several values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Dataset(object):\n",
    "    dataset_count = 0\n",
    "\n",
    "    def __init__(self, directory, subdir):\n",
    "        # deal with any intervening directories\n",
    "        while True:\n",
    "            subdirs = next(os.walk(directory))[1]\n",
    "            if len(subdirs) == 1:\n",
    "                directory = os.path.join(directory, subdirs[0])\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        slices = []\n",
    "        for s in subdirs:\n",
    "            m = re.match('sax_(\\d+)', s)\n",
    "            if m is not None:\n",
    "                slices.append(int(m.group(1)))\n",
    "\n",
    "        slices_map = {}\n",
    "        first = True\n",
    "        times = []\n",
    "        for s in slices:\n",
    "            files = next(os.walk(os.path.join(directory, 'sax_%d' % s)))[2]\n",
    "            offset = None\n",
    "\n",
    "            for f in files:\n",
    "                m = re.match('IM-(\\d{4,})-(\\d{4})\\.dcm', f)\n",
    "                if m is not None:\n",
    "                    if first:\n",
    "                        times.append(int(m.group(2)))\n",
    "                    if offset is None:\n",
    "                        offset = int(m.group(1))\n",
    "\n",
    "            first = False\n",
    "            slices_map[s] = offset\n",
    "\n",
    "        self.directory = directory\n",
    "        self.time = sorted(times)\n",
    "        self.slices = sorted(slices)\n",
    "        self.slices_map = slices_map\n",
    "        Dataset.dataset_count += 1\n",
    "        self.name = subdir\n",
    "\n",
    "    def _filename(self, s, t):\n",
    "        return os.path.join(self.directory,\n",
    "                            'sax_%d' % s,\n",
    "                            'IM-%04d-%04d.dcm' % (self.slices_map[s], t))\n",
    "\n",
    "    def _read_dicom_image(self, filename):\n",
    "        d = dicom.read_file(filename)\n",
    "        img = d.pixel_array.astype('int')\n",
    "        return img\n",
    "\n",
    "    def _read_all_dicom_images(self):\n",
    "        f1 = self._filename(self.slices[0], self.time[0])\n",
    "        d1 = dicom.read_file(f1)\n",
    "        (x, y) = d1.PixelSpacing\n",
    "        (x, y) = (float(x), float(y))\n",
    "        f2 = self._filename(self.slices[1], self.time[0])\n",
    "        d2 = dicom.read_file(f2)\n",
    "\n",
    "        # try a couple of things to measure distance between slices\n",
    "        try:\n",
    "            dist = np.abs(d2.SliceLocation - d1.SliceLocation)\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                dist = d1.SliceThickness\n",
    "            except AttributeError:\n",
    "                dist = 8  # better than nothing...\n",
    "\n",
    "        self.images = np.array([[self._read_dicom_image(self._filename(d, i))\n",
    "                                 for i in self.time]\n",
    "                                for d in self.slices])\n",
    "        self.dist = dist\n",
    "        self.area_multiplier = x * y\n",
    "\n",
    "    def load(self):\n",
    "        self._read_all_dicom_images()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is loaded using the `Dataset` class, we specify an operation to perform end-to-end LV segmentation and EF calculation in the function `segment_dataset`, which calls the following helper functions to accomplish the task:\n",
    "\n",
    "  * `calc_all_areas` function applies the trained Caffe FCN model to every image at the corresponding systole/diastole cycle or time to extract the LV contour. The pixels in the extracted LV contour are counted for utilization in subsequent volume calculation. Note that we leverage the [`Net::Reshape`](https://github.com/BVLC/caffe/pull/594) method in Caffe to dynamically resize the net, and reuse allocated memory accordingly, for every input image of arbitrary dimensions. In addition, our FCN model is a sliding window feature detector, so there is no need to resize input images to fixed dimensions, as is the common case for standard convolutional neural networks with fully connected layers.\n",
    "  * `calc_total_volume` function computes the volume of each LV contour (in milliliter) at each time slice using `area_multiplier` and the DICOM metadata `dist`. Volumes are computed for end diastole cycle (EDV) and for end systole cycle (ESV). In keeping with the idea that individual slices of the left ventricle are roughly circular, we model the volumes bounded by them as frustums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MEAN_VALUE = 77\n",
    "THRESH = 0.5\n",
    "\n",
    "def calc_all_areas(images):\n",
    "    (num_images, times, _, _) = images.shape\n",
    "    \n",
    "    all_masks = [{} for i in range(times)]\n",
    "    all_areas = [{} for i in range(times)]\n",
    "    for i in range(times):\n",
    "        for j in range(num_images):\n",
    "            # print 'Calculating area for time %d and slice %d...' % (i, j)\n",
    "            img = images[j][i]\n",
    "            in_ = np.expand_dims(img, axis=0)\n",
    "            in_ -= np.array([MEAN_VALUE])\n",
    "            net.blobs['data'].reshape(1, *in_.shape)\n",
    "            net.blobs['data'].data[...] = in_\n",
    "            net.forward()\n",
    "            prob = net.blobs['prob'].data\n",
    "            obj = prob[0][1]\n",
    "            preds = np.where(obj > THRESH, 1, 0)\n",
    "            all_masks[i][j] = preds\n",
    "            all_areas[i][j] = np.count_nonzero(preds)\n",
    "\n",
    "    return all_masks, all_areas\n",
    "\n",
    "def calc_total_volume(areas, area_multiplier, dist):\n",
    "    slices = np.array(sorted(areas.keys()))\n",
    "    modified = [areas[i] * area_multiplier for i in slices]\n",
    "    vol = 0\n",
    "    for i in slices[:-1]:\n",
    "        a, b = modified[i], modified[i+1]\n",
    "        subvol = (dist/3.0) * (a + np.sqrt(a*b) + b)\n",
    "        vol += subvol / 1000.0  # conversion to mL\n",
    "    return vol\n",
    "\n",
    "def segment_dataset(dataset):\n",
    "    # shape: num slices, num snapshots, rows, columns\n",
    "    print 'Calculating areas...'\n",
    "    all_masks, all_areas = calc_all_areas(dataset.images)\n",
    "    print 'Calculating volumes...'\n",
    "    area_totals = [calc_total_volume(a, dataset.area_multiplier, dataset.dist)\n",
    "                   for a in all_areas]\n",
    "    print 'Calculating EF...'\n",
    "    edv = max(area_totals)\n",
    "    esv = min(area_totals)\n",
    "    ef = (edv - esv) / edv\n",
    "    print 'Done, EF is {:0.4f}'.format(ef)\n",
    "    \n",
    "    dataset.edv = edv\n",
    "    dataset.esv = esv\n",
    "    dataset.ef = ef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, now that we have defined our helper functions, we can succinctly express the process of loading, segmenting, and scoring the dataset in the following code snippet to produce a file in comma-separated values for further analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# We capture all standard output from IPython so it does not flood the interface.\n",
    "with io.capture_output() as captured:\n",
    "    # edit this so it matches where you download the DSB data\n",
    "    DATA_PATH = '/home/jinjing/Desktop/dataset/'\n",
    "\n",
    "    caffe.set_mode_gpu()\n",
    "    net = caffe.('fcn_deploy.prototxt', './model_logs/fcn_iter_15000.caffemodel', caffe.TEST)\n",
    "\n",
    "    train_dir = os.path.join(DATA_PATH, 'train')\n",
    "    studies = next(os.walk(train_dir))[1]\n",
    "\n",
    "    labels = np.loadtxt(os.path.join(DATA_PATH, 'train.csv'), delimiter=',',\n",
    "                        skiprows=1)\n",
    "\n",
    "    label_map = {}\n",
    "    for l in labels:\n",
    "        label_map[l[0]] = (l[2], l[1])\n",
    "\n",
    "    if os.path.exists('output'):\n",
    "        shutil.rmtree('output')\n",
    "    os.mkdir('output')\n",
    "\n",
    "    accuracy_csv = open('accuracy.csv', 'w')\n",
    "\n",
    "    for s in studies:\n",
    "        dset = Dataset(os.path.join(train_dir, s), s)\n",
    "        print 'Processing dataset %s...' % dset.name\n",
    "        try:\n",
    "            dset.load()\n",
    "            segment_dataset(dset)\n",
    "            (edv, esv) = label_map[int(dset.name)]\n",
    "            accuracy_csv.write('%s,%f,%f,%f,%f\\n' %\n",
    "                               (dset.name, edv, esv, dset.edv, dset.esv))\n",
    "        except Exception as e:\n",
    "            print '***ERROR***: Exception %s thrown by dataset %s' % (str(e), dset.name)\n",
    "\n",
    "    accuracy_csv.close()\n",
    "\n",
    "# We redirect the captured stdout to a log file on disk.\n",
    "# This log file is very useful in identifying potential dataset irregularities that throw errors/exceptions in the code.\n",
    "with open('logs.txt', 'w') as f:\n",
    "    f.write(captured.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It takes about 75 minutes to apply the FCN model on all DICOM images in the DSB training set, extract LV contours, compute EDV/ESV volumes, and calculate EF on a MacBook Pro using NVIDIA GeForce GT 750M with 2GB of dedicated GPU memory. We can reduce the processing time if we enable the code sections above to perform *batch* processing. The code is currently configured to call Caffe and transfer data between CPU and GPU for *each* image. While data processing on the GPU is extremely fast, the constant data communication overhead negates some of the speed benefits. Batch processing can be done, for example, by batching all DICOM images belonging to a SAX study in a big NumPy stack and calling Caffe via `net.forward_all()` to load the NumPy stack into the GPU for batch processing. Although enabling batch processing will likely complicate the code, keep in mind that speed and efficiency will become critical as your model grows in complexity and size; it can be discouraging to try many experiments if an experiment takes a few hours to complete. Finally, remember to check the file `logs.txt`. This code is equipped to handle exceptions and errors associated with the DSB dataset. Upon inspection of the logs, we discover that a good amount of examples in the DSB training set contain irregularities. It is also likely that the validation and testing sets contain similar irregularities. The reader should do their best to handle these situations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Evaluate performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3 produces the file `accuracy.csv`, which consists of five columns with headers: IDs, actual EDV, actual ESV, predicted EDV, predicted ESV. The code below calculates actual and predicted EF from the EDV and ESV fields, through the simple relation `EF = (EDV - ESV) / EDV`, and computes some commonly used error metrics for the assessment of our model's predictive performance. We ignore (filter out) instances where the FCN model predicts zero values for EDV because we cannot derive a predicted EF value. I recommend the following strategies to remedy the shortcoming:\n",
    "* Impute the values for EDV, ESV, or both in instances where there are no predictions through interpolation: sample mean/median or through sophisticated polynomial interpolation using Python `scipy.interpolate`.\n",
    "* Continue improving the FCN model via tweaking and refinement. The idea is that as the model becomes more accurate in its capability to segment LV contours across the Sunnybrook and DSB datasets, then it is able to predict EDV/ESV/EF values with more precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calculate some error metrics to evaluate actual vs. predicted EF values obtained from FCN model\n",
    "data = np.transpose(np.loadtxt('accuracy.csv', delimiter=',')).astype('float')\n",
    "ids, actual_edv, actual_esv, predicted_edv, predicted_esv = data\n",
    "actual_ef = (actual_edv - actual_esv) / actual_edv\n",
    "actual_ef_std = np.std(actual_ef)\n",
    "actual_ef_median = np.median(actual_ef)\n",
    "predicted_ef = (predicted_edv - predicted_esv) / predicted_edv # potential of dividing by zero, where there is no predicted EDV value\n",
    "nan_idx = np.isnan(predicted_ef)\n",
    "actual_ef = actual_ef[~nan_idx]\n",
    "predicted_ef = predicted_ef[~nan_idx]\n",
    "MAE = np.mean(np.abs(actual_ef - predicted_ef))\n",
    "RMSE = np.sqrt(np.mean((actual_ef - predicted_ef)**2))\n",
    "print 'Mean absolute error (MAE) for predicted EF: {:0.4f}'.format(MAE)\n",
    "print 'Root mean square error (RMSE) for predicted EF: {:0.4f}'.format(RMSE)\n",
    "print 'Standard deviation of actual EF: {:0.4f}'.format(actual_ef_std)\n",
    "print 'Median value of actual EF: {:0.4f}'.format(actual_ef_median)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The FCN model achieves a mean absolute error (MAE) of `0.1796` and a root mean square error (RMSE) of `0.2265` for predicting ejection fraction (EF) on the DSB training set. In order to establish a frame of reference for these error values, we compare them to errors obtained from uniform random EF predictions (MAE = 0.2623, RMSE = 0.3128). However, if instead we use the median value of actual EF as reference predictions (MAE = 0.0776, RMSE = 0.1096), then our FCN results do not seem great in comparison. Let us not give up in despair. This is not a bad result considering the FCN model has not seen one image from the DSB dataset. The good news here is that our *baseline* FCN model is just the tip of the iceberg; there are many ways one can significantly improve upon this initial result (hint hint), and this is where the excitement of the competition ultimately lies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some ideas for improvement\n",
    "That's all, folks! This tutorial has shown how one can apply a fully convolutional network to segment the left ventricle from MRI images and to compute important metrics critical for diagnosing cardiovascular health. However, there is still a tremendous amount of work left to be done. Below are some ideas to consider for improving the performance of our baseline fully convolutional network:\n",
    "\n",
    "1. Expand the size of the network by increasing depth, width, or both.\n",
    "2. Explore different learning rates, weight initialization strategies, solver types, kernel sizes, strides, dropout ratio, and other network hyper-parameters that could positively impact its learning performance.\n",
    "3. Implement alternative pooling layers, such as cyclic pooling or fractional max-pooling.\n",
    "4. Investigate whether or not other non-linearities help improve performance, such as leaky ReLUs, parametric ReLUs, or exponential linear units (ELUs).\n",
    "5. Combine multiple models through bagging, boosting, stacking, and other methods in ensemble learning to improve predictive performance.\n",
    "6. Try out your own novel idea; this is the perfect platform to do so.\n",
    "\n",
    "I hope you find this tutorial useful in getting you started in the competition. Good luck on your quest to win a prize. I am anxious to see what innovative ideas will come out of this year's Data Science Bowl."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "The following references on Caffe may be useful:\n",
    "\n",
    "* Deconvolution layer - https://github.com/BVLC/caffe/pull/1615\n",
    "* Crop layer - https://github.com/BVLC/caffe/pull/1976\n",
    "* Bilinear upsampling - https://github.com/BVLC/caffe/pull/2213\n",
    "* On-the-fly net resizing - https://github.com/BVLC/caffe/pull/594\n",
    "* Main Caffe tutorial - http://caffe.berkeleyvision.org/tutorial/\n",
    "* Caffe examples - http://caffe.berkeleyvision.org/\n",
    "* Caffe net specification - https://github.com/BVLC/caffe/blob/master/src/caffe/proto/caffe.proto\n",
    "* Caffe users group - https://groups.google.com/forum/#!forum/caffe-users\n",
    "* Caffe GitHub page - https://github.com/BVLC/caffe\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
